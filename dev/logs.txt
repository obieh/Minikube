
==> Audit <==
|--------------|-------------------------------|----------|------|---------|---------------------|---------------------|
|   Command    |             Args              | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|-------------------------------|----------|------|---------|---------------------|---------------------|
| update-check |                               | minikube | obie | v1.28.0 | 18 Jan 23 00:47 WAT | 18 Jan 23 00:47 WAT |
| start        | --download-only --output json | minikube | obie | v1.28.0 | 24 Jan 23 13:59 WAT |                     |
| update-check |                               | minikube | obie | v1.28.0 | 24 Jan 23 14:20 WAT | 24 Jan 23 14:20 WAT |
| update-check |                               | minikube | obie | v1.28.0 | 25 Jan 23 01:21 WAT | 25 Jan 23 01:21 WAT |
| start        | --download-only --output json | minikube | obie | v1.28.0 | 25 Jan 23 01:25 WAT |                     |
| update-check |                               | minikube | obie | v1.28.0 | 28 Jan 23 20:36 WAT | 28 Jan 23 20:36 WAT |
| start        | --download-only --output json | minikube | obie | v1.28.0 | 28 Jan 23 20:39 WAT |                     |
| start        | --download-only --output json | minikube | obie | v1.29.0 | 17 Feb 23 08:12 WAT |                     |
| start        | --download-only --output json | minikube | obie | v1.29.0 | 17 Feb 23 08:18 WAT |                     |
| start        | --download-only --output json | minikube | obie | v1.29.0 | 17 Feb 23 08:43 WAT |                     |
| update-check |                               | minikube | obie | v1.28.0 | 18 Feb 23 03:11 WAT | 18 Feb 23 03:12 WAT |
| start        | --download-only --output json | minikube | obie | v1.29.0 | 18 Feb 23 03:14 WAT |                     |
| start        | --download-only --output json | minikube | obie | v1.29.0 | 22 Feb 23 03:55 WAT |                     |
| update-check |                               | minikube | obie | v1.28.0 | 01 Mar 23 15:28 WAT | 01 Mar 23 15:28 WAT |
| start        |                               | minikube | obie | v1.28.0 | 17 Mar 23 09:12 WAT | 17 Mar 23 09:12 WAT |
| start        |                               | minikube | obie | v1.28.0 | 17 Mar 23 18:15 WAT | 17 Mar 23 18:16 WAT |
| start        |                               | minikube | obie | v1.28.0 | 18 Mar 23 17:08 WAT | 18 Mar 23 17:08 WAT |
| start        |                               | minikube | obie | v1.28.0 | 18 Mar 23 23:23 WAT | 18 Mar 23 23:23 WAT |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:22 WAT |                     |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:26 WAT |                     |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:29 WAT |                     |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:30 WAT |                     |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:35 WAT |                     |
| start        |                               | minikube | obie | v1.28.0 | 26 Jun 24 16:49 WAT |                     |
| start        | --driver=docker               | minikube | obie | v1.28.0 | 26 Jun 24 17:03 WAT |                     |
| start        | --driver=docker               | minikube | obie | v1.33.1 | 26 Jun 24 17:11 WAT | 26 Jun 24 17:11 WAT |
| start        |                               | minikube | obie | v1.33.1 | 26 Jun 24 17:12 WAT | 26 Jun 24 17:12 WAT |
| start        |                               | minikube | obie | v1.33.1 | 26 Jun 24 17:28 WAT | 26 Jun 24 17:28 WAT |
| stop         |                               | minikube | obie | v1.33.1 | 26 Jun 24 17:37 WAT | 26 Jun 24 17:37 WAT |
| start        |                               | minikube | obie | v1.33.1 | 27 Jun 24 15:24 WAT | 27 Jun 24 15:24 WAT |
| update-check |                               | minikube | obie | v1.33.1 | 27 Jun 24 15:45 WAT | 27 Jun 24 15:46 WAT |
| service      | nodeapp-service               | minikube | obie | v1.33.1 | 27 Jun 24 16:11 WAT |                     |
| service      | nodeapp-service               | minikube | obie | v1.33.1 | 27 Jun 24 16:49 WAT | 27 Jun 24 16:49 WAT |
| dashboard    |                               | minikube | obie | v1.33.1 | 27 Jun 24 16:50 WAT |                     |
| service      | nodeapp-service               | minikube | obie | v1.33.1 | 27 Jun 24 16:55 WAT | 27 Jun 24 16:55 WAT |
| service      | nodeapp-service --url         | minikube | obie | v1.33.1 | 27 Jun 24 17:26 WAT | 27 Jun 24 17:26 WAT |
| dashboard    |                               | minikube | obie | v1.33.1 | 27 Jun 24 17:48 WAT |                     |
| update-check |                               | minikube | obie | v1.33.1 | 28 Jun 24 10:15 WAT | 28 Jun 24 10:15 WAT |
| start        |                               | minikube | obie | v1.33.1 | 28 Jun 24 10:15 WAT | 28 Jun 24 10:15 WAT |
| start        |                               | minikube | obie | v1.33.1 | 28 Jun 24 16:36 WAT | 28 Jun 24 16:37 WAT |
| service      | nodeapp-service --url         | minikube | obie | v1.33.1 | 28 Jun 24 16:45 WAT | 28 Jun 24 16:45 WAT |
| update-check |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:03 WAT | 28 Jun 24 17:03 WAT |
| ip           |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:05 WAT | 28 Jun 24 17:05 WAT |
| stop         |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:06 WAT | 28 Jun 24 17:06 WAT |
| start        |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:08 WAT | 28 Jun 24 17:08 WAT |
| stop         |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:11 WAT | 28 Jun 24 17:11 WAT |
| start        |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:12 WAT | 28 Jun 24 17:12 WAT |
| ip           |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:20 WAT | 28 Jun 24 17:20 WAT |
| service      | nodeapp-service --url         | minikube | obie | v1.33.1 | 28 Jun 24 17:21 WAT | 28 Jun 24 17:21 WAT |
| ip           |                               | minikube | obie | v1.33.1 | 28 Jun 24 17:49 WAT | 28 Jun 24 17:49 WAT |
| dashboard    |                               | minikube | obie | v1.33.1 | 28 Jun 24 18:15 WAT |                     |
| service      | nodeapp-service --url         | minikube | obie | v1.33.1 | 28 Jun 24 18:22 WAT | 28 Jun 24 18:22 WAT |
| stop         |                               | minikube | obie | v1.33.1 | 28 Jun 24 18:46 WAT | 28 Jun 24 18:47 WAT |
| update-check |                               | minikube | obie | v1.33.1 | 30 Jun 24 07:11 WAT |                     |
| start        |                               | minikube | obie | v1.33.1 | 30 Jun 24 15:36 WAT | 30 Jun 24 15:37 WAT |
| update-check |                               | minikube | obie | v1.33.1 | 30 Jun 24 15:46 WAT | 30 Jun 24 15:46 WAT |
| service      | jenkinsappsvc --namespace=dev | minikube | obie | v1.33.1 | 30 Jun 24 17:20 WAT |                     |
| stop         |                               | minikube | obie | v1.33.1 | 30 Jun 24 17:31 WAT | 30 Jun 24 17:31 WAT |
| start        |                               | minikube | obie | v1.33.1 | 30 Jun 24 17:32 WAT | 30 Jun 24 17:32 WAT |
| service      | jenkinsappsvc --namespace=dev | minikube | obie | v1.33.1 | 30 Jun 24 17:34 WAT |                     |
|--------------|-------------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/30 17:32:01
Running on machine: shenko
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0630 17:32:01.296366  216326 out.go:291] Setting OutFile to fd 1 ...
I0630 17:32:01.296458  216326 out.go:343] isatty.IsTerminal(1) = true
I0630 17:32:01.296463  216326 out.go:304] Setting ErrFile to fd 2...
I0630 17:32:01.296469  216326 out.go:343] isatty.IsTerminal(2) = true
I0630 17:32:01.296631  216326 root.go:338] Updating PATH: /home/obie/.minikube/bin
W0630 17:32:01.296716  216326 root.go:314] Error reading config file at /home/obie/.minikube/config/config.json: open /home/obie/.minikube/config/config.json: no such file or directory
I0630 17:32:01.296946  216326 out.go:298] Setting JSON to false
I0630 17:32:01.298280  216326 start.go:129] hostinfo: {"hostname":"shenko","uptime":13197,"bootTime":1719751924,"procs":387,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-41-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"b9f19465-a140-4624-b9a1-4916240c0198"}
I0630 17:32:01.298333  216326 start.go:139] virtualization: kvm host
I0630 17:32:01.300609  216326 out.go:177] 😄  minikube v1.33.1 on Ubuntu 22.04
I0630 17:32:01.303696  216326 notify.go:220] Checking for updates...
I0630 17:32:01.303980  216326 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0630 17:32:01.305704  216326 out.go:177] 🆕  Kubernetes 1.30.0 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.30.0
I0630 17:32:01.307242  216326 driver.go:392] Setting default libvirt URI to qemu:///system
I0630 17:32:01.324315  216326 docker.go:122] docker version: linux-27.0.1:Docker Engine - Community
I0630 17:32:01.324390  216326 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0630 17:32:01.364983  216326 info.go:266] docker info: {ID:USMC:3LL5:N5VD:JSHN:UOZ6:GI4N:HNCP:6IQL:VFN3:TJFI:GTNQ:E3G6 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:28 OomKillDisable:false NGoroutines:40 SystemTime:2024-06-30 17:32:01.358349064 +0100 WAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:33445560320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shenko Labels:[] ExperimentalBuild:false ServerVersion:27.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0630 17:32:01.365058  216326 docker.go:295] overlay module found
I0630 17:32:01.366728  216326 out.go:177] ✨  Using the docker driver based on existing profile
I0630 17:32:01.368226  216326 start.go:297] selected driver: docker
I0630 17:32:01.368231  216326 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/obie:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:0s}
I0630 17:32:01.368323  216326 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0630 17:32:01.368386  216326 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0630 17:32:01.404465  216326 info.go:266] docker info: {ID:USMC:3LL5:N5VD:JSHN:UOZ6:GI4N:HNCP:6IQL:VFN3:TJFI:GTNQ:E3G6 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:28 OomKillDisable:false NGoroutines:40 SystemTime:2024-06-30 17:32:01.396941278 +0100 WAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:33445560320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shenko Labels:[] ExperimentalBuild:false ServerVersion:27.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0630 17:32:01.405252  216326 cni.go:84] Creating CNI manager for ""
I0630 17:32:01.405266  216326 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0630 17:32:01.405319  216326 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/obie:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:0s}
I0630 17:32:01.407414  216326 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0630 17:32:01.410302  216326 cache.go:121] Beginning downloading kic base image for docker with docker
I0630 17:32:01.411827  216326 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0630 17:32:01.414613  216326 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0630 17:32:01.414731  216326 preload.go:147] Found local preload: /home/obie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I0630 17:32:01.414758  216326 cache.go:56] Caching tarball of preloaded images
I0630 17:32:01.414763  216326 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.36 in local docker daemon
I0630 17:32:01.415027  216326 preload.go:173] Found /home/obie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0630 17:32:01.415066  216326 cache.go:59] Finished verifying existence of preloaded tar for v1.25.3 on docker
I0630 17:32:01.415482  216326 profile.go:143] Saving config to /home/obie/.minikube/profiles/minikube/config.json ...
I0630 17:32:01.448356  216326 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.36 in local docker daemon, skipping pull
I0630 17:32:01.448391  216326 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.36 exists in daemon, skipping load
I0630 17:32:01.448409  216326 cache.go:194] Successfully downloaded all kic artifacts
I0630 17:32:01.448478  216326 start.go:360] acquireMachinesLock for minikube: {Name:mkb93ac0e0ba5ba8d2c1f0cb374dd5c50fa3274e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0630 17:32:01.448575  216326 start.go:364] duration metric: took 69.66µs to acquireMachinesLock for "minikube"
I0630 17:32:01.448594  216326 start.go:96] Skipping create...Using existing machine configuration
I0630 17:32:01.448598  216326 fix.go:54] fixHost starting: 
I0630 17:32:01.448909  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:01.464168  216326 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0630 17:32:01.464187  216326 fix.go:138] unexpected machine state, will restart: <nil>
I0630 17:32:01.466069  216326 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0630 17:32:01.467627  216326 cli_runner.go:164] Run: docker start minikube
I0630 17:32:01.835436  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:01.878054  216326 kic.go:430] container "minikube" state is running.
I0630 17:32:01.878804  216326 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0630 17:32:01.910500  216326 profile.go:143] Saving config to /home/obie/.minikube/profiles/minikube/config.json ...
I0630 17:32:01.910969  216326 machine.go:94] provisionDockerMachine start ...
I0630 17:32:01.911068  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:01.943629  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:01.944101  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:01.944117  216326 main.go:141] libmachine: About to run SSH command:
hostname
I0630 17:32:01.944980  216326 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:36716->127.0.0.1:32773: read: connection reset by peer
I0630 17:32:05.095394  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0630 17:32:05.095460  216326 ubuntu.go:169] provisioning hostname "minikube"
I0630 17:32:05.095611  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:05.131922  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:05.132160  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:05.132171  216326 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0630 17:32:05.316569  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0630 17:32:05.316661  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:05.331058  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:05.331214  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:05.331225  216326 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0630 17:32:05.464977  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0630 17:32:05.465000  216326 ubuntu.go:175] set auth options {CertDir:/home/obie/.minikube CaCertPath:/home/obie/.minikube/certs/ca.pem CaPrivateKeyPath:/home/obie/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/obie/.minikube/machines/server.pem ServerKeyPath:/home/obie/.minikube/machines/server-key.pem ClientKeyPath:/home/obie/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/obie/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/obie/.minikube}
I0630 17:32:05.465020  216326 ubuntu.go:177] setting up certificates
I0630 17:32:05.465029  216326 provision.go:84] configureAuth start
I0630 17:32:05.465102  216326 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0630 17:32:05.482792  216326 provision.go:143] copyHostCerts
I0630 17:32:05.482838  216326 exec_runner.go:144] found /home/obie/.minikube/ca.pem, removing ...
I0630 17:32:05.482844  216326 exec_runner.go:203] rm: /home/obie/.minikube/ca.pem
I0630 17:32:05.482912  216326 exec_runner.go:151] cp: /home/obie/.minikube/certs/ca.pem --> /home/obie/.minikube/ca.pem (1070 bytes)
I0630 17:32:05.483027  216326 exec_runner.go:144] found /home/obie/.minikube/cert.pem, removing ...
I0630 17:32:05.483032  216326 exec_runner.go:203] rm: /home/obie/.minikube/cert.pem
I0630 17:32:05.483073  216326 exec_runner.go:151] cp: /home/obie/.minikube/certs/cert.pem --> /home/obie/.minikube/cert.pem (1115 bytes)
I0630 17:32:05.483141  216326 exec_runner.go:144] found /home/obie/.minikube/key.pem, removing ...
I0630 17:32:05.483146  216326 exec_runner.go:203] rm: /home/obie/.minikube/key.pem
I0630 17:32:05.483179  216326 exec_runner.go:151] cp: /home/obie/.minikube/certs/key.pem --> /home/obie/.minikube/key.pem (1679 bytes)
I0630 17:32:05.483239  216326 provision.go:117] generating server cert: /home/obie/.minikube/machines/server.pem ca-key=/home/obie/.minikube/certs/ca.pem private-key=/home/obie/.minikube/certs/ca-key.pem org=obie.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0630 17:32:05.582723  216326 provision.go:177] copyRemoteCerts
I0630 17:32:05.582765  216326 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0630 17:32:05.582792  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:05.592807  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:05.682907  216326 ssh_runner.go:362] scp /home/obie/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0630 17:32:05.703955  216326 ssh_runner.go:362] scp /home/obie/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0630 17:32:05.723474  216326 ssh_runner.go:362] scp /home/obie/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0630 17:32:05.742980  216326 provision.go:87] duration metric: took 277.942373ms to configureAuth
I0630 17:32:05.742993  216326 ubuntu.go:193] setting minikube options for container-runtime
I0630 17:32:05.743155  216326 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0630 17:32:05.743193  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:05.756992  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:05.757147  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:05.757154  216326 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0630 17:32:05.907788  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0630 17:32:05.907817  216326 ubuntu.go:71] root file system type: overlay
I0630 17:32:05.908053  216326 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0630 17:32:05.908176  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:05.942582  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:05.942912  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:05.943058  216326 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0630 17:32:06.129218  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0630 17:32:06.129335  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:06.149417  216326 main.go:141] libmachine: Using SSH client type: native
I0630 17:32:06.149650  216326 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0630 17:32:06.149675  216326 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0630 17:32:06.297893  216326 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0630 17:32:06.301193  216326 machine.go:97] duration metric: took 4.390207721s to provisionDockerMachine
I0630 17:32:06.301208  216326 start.go:293] postStartSetup for "minikube" (driver="docker")
I0630 17:32:06.301220  216326 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0630 17:32:06.301335  216326 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0630 17:32:06.301395  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:06.319499  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:06.432672  216326 ssh_runner.go:195] Run: cat /etc/os-release
I0630 17:32:06.437463  216326 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0630 17:32:06.437478  216326 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0630 17:32:06.437488  216326 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0630 17:32:06.437495  216326 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0630 17:32:06.437504  216326 filesync.go:126] Scanning /home/obie/.minikube/addons for local assets ...
I0630 17:32:06.437557  216326 filesync.go:126] Scanning /home/obie/.minikube/files for local assets ...
I0630 17:32:06.437580  216326 start.go:296] duration metric: took 136.36573ms for postStartSetup
I0630 17:32:06.437627  216326 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0630 17:32:06.437667  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:06.454553  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:06.555884  216326 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0630 17:32:06.571250  216326 fix.go:56] duration metric: took 5.122627024s for fixHost
I0630 17:32:06.571305  216326 start.go:83] releasing machines lock for "minikube", held for 5.122710222s
I0630 17:32:06.571537  216326 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0630 17:32:06.602362  216326 ssh_runner.go:195] Run: cat /version.json
I0630 17:32:06.602403  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:06.602418  216326 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0630 17:32:06.602466  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:06.620867  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:06.621203  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
W0630 17:32:07.487969  216326 start.go:419] Unable to open version.json: cat /version.json: Process exited with status 1
stdout:

stderr:
cat: /version.json: No such file or directory
I0630 17:32:07.488156  216326 ssh_runner.go:195] Run: systemctl --version
I0630 17:32:07.503444  216326 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0630 17:32:07.519723  216326 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0630 17:32:07.566670  216326 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0630 17:32:07.566786  216326 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *bridge* -not -name *podman* -not -name *.mk_disabled -printf "%!p(MISSING), " -exec sh -c "sudo sed -i -r -e '/"dst": ".*:.*"/d' -e 's|^(.*)"dst": (.*)[,*]$|\1"dst": \2|g' -e '/"subnet": ".*:.*"/d' -e 's|^(.*)"subnet": ".*"(.*)[,*]$|\1"subnet": "10.244.0.0/16"\2|g' {}" ;
I0630 17:32:07.592152  216326 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *podman* -not -name *.mk_disabled -printf "%!p(MISSING), " -exec sh -c "sudo sed -i -r -e 's|^(.*)"subnet": ".*"(.*)$|\1"subnet": "10.244.0.0/16"\2|g' -e 's|^(.*)"gateway": ".*"(.*)$|\1"gateway": "10.244.0.1"\2|g' {}" ;
I0630 17:32:07.603400  216326 cni.go:308] configured [/etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0630 17:32:07.603417  216326 start.go:494] detecting cgroup driver to use...
I0630 17:32:07.603448  216326 detect.go:199] detected "systemd" cgroup driver on host os
I0630 17:32:07.603534  216326 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0630 17:32:07.621028  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.8"|' /etc/containerd/config.toml"
I0630 17:32:07.629453  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0630 17:32:07.637975  216326 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0630 17:32:07.638016  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0630 17:32:07.645973  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0630 17:32:07.654104  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0630 17:32:07.661707  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0630 17:32:07.669497  216326 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0630 17:32:07.676310  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0630 17:32:07.684614  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0630 17:32:07.692823  216326 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0630 17:32:07.700868  216326 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0630 17:32:07.707655  216326 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0630 17:32:07.714284  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:07.793902  216326 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0630 17:32:07.882086  216326 start.go:494] detecting cgroup driver to use...
I0630 17:32:07.882117  216326 detect.go:199] detected "systemd" cgroup driver on host os
I0630 17:32:07.882159  216326 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0630 17:32:07.919442  216326 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0630 17:32:07.919494  216326 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0630 17:32:07.928870  216326 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0630 17:32:07.942786  216326 ssh_runner.go:195] Run: which cri-dockerd
I0630 17:32:07.945421  216326 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0630 17:32:07.952325  216326 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0630 17:32:07.964318  216326 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0630 17:32:08.026711  216326 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0630 17:32:08.093340  216326 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0630 17:32:08.093407  216326 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0630 17:32:08.104229  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:08.163066  216326 ssh_runner.go:195] Run: sudo systemctl restart docker
I0630 17:32:10.117447  216326 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.954339725s)
I0630 17:32:10.117556  216326 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0630 17:32:10.138283  216326 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0630 17:32:10.150106  216326 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0630 17:32:10.158600  216326 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0630 17:32:10.226047  216326 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0630 17:32:10.317168  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:10.377875  216326 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0630 17:32:10.410757  216326 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0630 17:32:10.420141  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:10.495966  216326 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0630 17:32:10.592125  216326 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0630 17:32:10.592164  216326 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0630 17:32:10.594972  216326 start.go:562] Will wait 60s for crictl version
I0630 17:32:10.595008  216326 ssh_runner.go:195] Run: which crictl
I0630 17:32:10.597353  216326 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0630 17:32:10.622191  216326 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I0630 17:32:10.622232  216326 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0630 17:32:10.643110  216326 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0630 17:32:10.666977  216326 out.go:204] 🐳  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I0630 17:32:10.667057  216326 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0630 17:32:10.678597  216326 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0630 17:32:10.681816  216326 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0630 17:32:10.692715  216326 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/obie:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:0s} ...
I0630 17:32:10.692851  216326 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I0630 17:32:10.692900  216326 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0630 17:32:10.714407  216326 docker.go:685] Got preloaded images: -- stdout --
obieh/node.test:latest
nginx:latest
mongo-express:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
<none>:<none>
<none>:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0630 17:32:10.714415  216326 docker.go:615] Images already preloaded, skipping extraction
I0630 17:32:10.714467  216326 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0630 17:32:10.735455  216326 docker.go:685] Got preloaded images: -- stdout --
obieh/node.test:latest
nginx:latest
mongo-express:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
<none>:<none>
<none>:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0630 17:32:10.735465  216326 cache_images.go:84] Images are preloaded, skipping loading
I0630 17:32:10.735471  216326 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.25.3 docker true true} ...
I0630 17:32:10.735543  216326 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0630 17:32:10.735587  216326 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0630 17:32:10.794505  216326 cni.go:84] Creating CNI manager for ""
I0630 17:32:10.794517  216326 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0630 17:32:10.794523  216326 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0630 17:32:10.794539  216326 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0630 17:32:10.794640  216326 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0630 17:32:10.794684  216326 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I0630 17:32:10.802003  216326 binaries.go:44] Found k8s binaries, skipping transfer
I0630 17:32:10.802045  216326 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0630 17:32:10.808694  216326 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0630 17:32:10.821818  216326 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0630 17:32:10.834804  216326 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0630 17:32:10.848009  216326 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0630 17:32:10.851066  216326 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0630 17:32:10.860117  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:10.927832  216326 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0630 17:32:10.958638  216326 certs.go:68] Setting up /home/obie/.minikube/profiles/minikube for IP: 192.168.49.2
I0630 17:32:10.958645  216326 certs.go:194] generating shared ca certs ...
I0630 17:32:10.958655  216326 certs.go:226] acquiring lock for ca certs: {Name:mkdb1bb491cdcf6d32b2aa7c407dd654b59d5321 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0630 17:32:10.958757  216326 certs.go:235] skipping valid "minikubeCA" ca cert: /home/obie/.minikube/ca.key
I0630 17:32:10.958799  216326 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/obie/.minikube/proxy-client-ca.key
I0630 17:32:10.958808  216326 certs.go:256] generating profile certs ...
I0630 17:32:10.958885  216326 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/obie/.minikube/profiles/minikube/client.key
I0630 17:32:10.958932  216326 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/obie/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0630 17:32:10.958974  216326 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/obie/.minikube/profiles/minikube/proxy-client.key
I0630 17:32:10.959070  216326 certs.go:484] found cert: /home/obie/.minikube/certs/ca-key.pem (1675 bytes)
I0630 17:32:10.959098  216326 certs.go:484] found cert: /home/obie/.minikube/certs/ca.pem (1070 bytes)
I0630 17:32:10.959120  216326 certs.go:484] found cert: /home/obie/.minikube/certs/cert.pem (1115 bytes)
I0630 17:32:10.959141  216326 certs.go:484] found cert: /home/obie/.minikube/certs/key.pem (1679 bytes)
I0630 17:32:10.959736  216326 ssh_runner.go:362] scp /home/obie/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0630 17:32:10.978275  216326 ssh_runner.go:362] scp /home/obie/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0630 17:32:10.997292  216326 ssh_runner.go:362] scp /home/obie/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0630 17:32:11.017389  216326 ssh_runner.go:362] scp /home/obie/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0630 17:32:11.040849  216326 ssh_runner.go:362] scp /home/obie/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0630 17:32:11.057897  216326 ssh_runner.go:362] scp /home/obie/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0630 17:32:11.074738  216326 ssh_runner.go:362] scp /home/obie/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0630 17:32:11.095280  216326 ssh_runner.go:362] scp /home/obie/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0630 17:32:11.116787  216326 ssh_runner.go:362] scp /home/obie/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0630 17:32:11.137422  216326 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0630 17:32:11.151732  216326 ssh_runner.go:195] Run: openssl version
I0630 17:32:11.156750  216326 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0630 17:32:11.165083  216326 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0630 17:32:11.168184  216326 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov  5  2022 /usr/share/ca-certificates/minikubeCA.pem
I0630 17:32:11.168228  216326 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0630 17:32:11.173267  216326 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0630 17:32:11.181047  216326 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0630 17:32:11.184723  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0630 17:32:11.190298  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0630 17:32:11.196997  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0630 17:32:11.202735  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0630 17:32:11.208520  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0630 17:32:11.213671  216326 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0630 17:32:11.218579  216326 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/obie:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:0s}
I0630 17:32:11.218708  216326 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0630 17:32:11.239622  216326 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0630 17:32:11.246730  216326 host.go:66] Checking if "minikube" exists ...
I0630 17:32:11.246923  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:11.259462  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:11.272584  216326 main.go:141] libmachine: Using SSH client type: external
I0630 17:32:11.272607  216326 main.go:141] libmachine: Using SSH private key: /home/obie/.minikube/machines/minikube/id_rsa (-rw-------)
I0630 17:32:11.272635  216326 main.go:141] libmachine: &{[-F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@127.0.0.1 -o IdentitiesOnly=yes -i /home/obie/.minikube/machines/minikube/id_rsa -p 32773] /usr/bin/ssh <nil>}
I0630 17:32:11.272653  216326 main.go:141] libmachine: /usr/bin/ssh -F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@127.0.0.1 -o IdentitiesOnly=yes -i /home/obie/.minikube/machines/minikube/id_rsa -p 32773 -f -NTL 0:localhost:8443
W0630 17:32:11.277104  216326 kubeadm.go:404] apiserver tunnel failed: ssh command: exit status 255
I0630 17:32:11.277128  216326 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0630 17:32:11.277131  216326 kubeadm.go:587] restartPrimaryControlPlane start ...
I0630 17:32:11.277174  216326 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0630 17:32:11.285816  216326 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0630 17:32:11.286114  216326 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/obie/.kube/config
I0630 17:32:11.286186  216326 kubeconfig.go:62] /home/obie/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0630 17:32:11.286359  216326 lock.go:35] WriteFile acquiring /home/obie/.kube/config: {Name:mkc86e71038ad7f9c2212ac88ec1fbe6ef0b1c40 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0630 17:32:11.287397  216326 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0630 17:32:11.294700  216326 kubeadm.go:624] The running cluster does not require reconfiguration: 192.168.49.2
I0630 17:32:11.294720  216326 kubeadm.go:591] duration metric: took 17.583162ms to restartPrimaryControlPlane
I0630 17:32:11.294728  216326 kubeadm.go:393] duration metric: took 76.156382ms to StartCluster
I0630 17:32:11.294742  216326 settings.go:142] acquiring lock: {Name:mk3ca445263ed7635b65ad764c1f7072ed160186 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0630 17:32:11.294792  216326 settings.go:150] Updating kubeconfig:  /home/obie/.kube/config
I0630 17:32:11.295465  216326 lock.go:35] WriteFile acquiring /home/obie/.kube/config: {Name:mkc86e71038ad7f9c2212ac88ec1fbe6ef0b1c40 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0630 17:32:11.295651  216326 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0630 17:32:11.298114  216326 out.go:177] 🔎  Verifying Kubernetes components...
I0630 17:32:11.295739  216326 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0630 17:32:11.295852  216326 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I0630 17:32:11.299725  216326 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0630 17:32:11.301372  216326 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0630 17:32:11.299725  216326 addons.go:69] Setting dashboard=true in profile "minikube"
W0630 17:32:11.301384  216326 addons.go:243] addon storage-provisioner should already be in state true
I0630 17:32:11.301407  216326 addons.go:234] Setting addon dashboard=true in "minikube"
I0630 17:32:11.301411  216326 host.go:66] Checking if "minikube" exists ...
W0630 17:32:11.301416  216326 addons.go:243] addon dashboard should already be in state true
I0630 17:32:11.301439  216326 host.go:66] Checking if "minikube" exists ...
I0630 17:32:11.299733  216326 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0630 17:32:11.299737  216326 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0630 17:32:11.301519  216326 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0630 17:32:11.301750  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:11.301755  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:11.301877  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:11.320561  216326 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0630 17:32:11.324009  216326 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0630 17:32:11.321122  216326 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0630 17:32:11.323920  216326 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0630 17:32:11.326054  216326 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0630 17:32:11.328077  216326 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
W0630 17:32:11.326095  216326 addons.go:243] addon default-storageclass should already be in state true
I0630 17:32:11.326134  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:11.330903  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0630 17:32:11.330928  216326 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0630 17:32:11.331010  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:11.331050  216326 host.go:66] Checking if "minikube" exists ...
I0630 17:32:11.331496  216326 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0630 17:32:11.354457  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:11.356199  216326 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0630 17:32:11.356210  216326 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0630 17:32:11.356268  216326 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0630 17:32:11.359661  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:11.371830  216326 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/obie/.minikube/machines/minikube/id_rsa Username:docker}
I0630 17:32:11.454284  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0630 17:32:11.454295  216326 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0630 17:32:11.456502  216326 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0630 17:32:11.458752  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0630 17:32:11.467172  216326 api_server.go:52] waiting for apiserver process to appear ...
I0630 17:32:11.467228  216326 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0630 17:32:11.469064  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0630 17:32:11.469074  216326 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0630 17:32:11.471710  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0630 17:32:11.483799  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0630 17:32:11.483813  216326 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0630 17:32:11.499415  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0630 17:32:11.499423  216326 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0630 17:32:11.516406  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0630 17:32:11.516419  216326 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0630 17:32:11.532848  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0630 17:32:11.532860  216326 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
W0630 17:32:11.532949  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.533032  216326 retry.go:31] will retry after 209.939281ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
W0630 17:32:11.543121  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.543136  216326 retry.go:31] will retry after 274.887096ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.546031  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0630 17:32:11.546042  216326 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0630 17:32:11.559171  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0630 17:32:11.559191  216326 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0630 17:32:11.572290  216326 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0630 17:32:11.572299  216326 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0630 17:32:11.585262  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0630 17:32:11.629045  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.629064  216326 retry.go:31] will retry after 267.858598ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.743175  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0630 17:32:11.799658  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.799675  216326 retry.go:31] will retry after 318.022916ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.819012  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0630 17:32:11.862626  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.862638  216326 retry.go:31] will retry after 224.444632ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.898097  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0630 17:32:11.965709  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.965721  216326 retry.go:31] will retry after 420.271346ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:11.967853  216326 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0630 17:32:12.087637  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0630 17:32:12.118193  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0630 17:32:12.167686  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.167698  216326 retry.go:31] will retry after 608.740968ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
W0630 17:32:12.175519  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.175533  216326 retry.go:31] will retry after 328.955121ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.387340  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0630 17:32:12.467974  216326 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0630 17:32:12.503316  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.503329  216326 retry.go:31] will retry after 410.934071ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.505469  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0630 17:32:12.549234  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.549251  216326 retry.go:31] will retry after 681.734057ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.776584  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0630 17:32:12.864046  216326 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.864065  216326 retry.go:31] will retry after 732.785971ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
I0630 17:32:12.915293  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0630 17:32:12.967266  216326 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0630 17:32:13.231159  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0630 17:32:13.597014  216326 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0630 17:32:16.429677  216326 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.462392204s)
I0630 17:32:16.429679  216326 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.514366262s)
I0630 17:32:16.429692  216326 api_server.go:72] duration metric: took 5.134021924s to wait for apiserver process to appear ...
I0630 17:32:16.429695  216326 api_server.go:88] waiting for apiserver healthz status ...
I0630 17:32:16.429706  216326 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0630 17:32:16.429707  216326 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.198537754s)
I0630 17:32:16.431588  216326 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0630 17:32:16.429743  216326 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.832717335s)
I0630 17:32:16.432588  216326 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0630 17:32:16.433029  216326 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0630 17:32:16.443834  216326 out.go:177] 🌟  Enabled addons: storage-provisioner, dashboard, default-storageclass
I0630 17:32:16.445377  216326 addons.go:505] duration metric: took 5.149632421s for enable addons: enabled=[storage-provisioner dashboard default-storageclass]
I0630 17:32:16.930330  216326 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0630 17:32:16.937109  216326 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0630 17:32:16.937133  216326 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0630 17:32:17.430854  216326 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0630 17:32:17.447370  216326 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0630 17:32:17.450029  216326 api_server.go:141] control plane version: v1.25.3
I0630 17:32:17.450071  216326 api_server.go:131] duration metric: took 1.020363204s to wait for apiserver health ...
I0630 17:32:17.450089  216326 system_pods.go:43] waiting for kube-system pods to appear ...
I0630 17:32:17.476071  216326 system_pods.go:59] 7 kube-system pods found
I0630 17:32:17.476122  216326 system_pods.go:61] "coredns-565d847f94-22v8l" [4ebf06a5-ed53-40de-981b-ba0b541709ce] Running
I0630 17:32:17.476191  216326 system_pods.go:61] "etcd-minikube" [9f538057-5f9d-40b8-beec-02f607d6dcb9] Running
I0630 17:32:17.476210  216326 system_pods.go:61] "kube-apiserver-minikube" [bf8be4fb-40ef-41ca-b905-bdd149de6ade] Running
I0630 17:32:17.476227  216326 system_pods.go:61] "kube-controller-manager-minikube" [0fce4c01-cec2-4070-bb40-beb2da70db12] Running
I0630 17:32:17.476239  216326 system_pods.go:61] "kube-proxy-z52gj" [b7a906b7-2556-4e1b-85a9-0ec50283c9cf] Running
I0630 17:32:17.476261  216326 system_pods.go:61] "kube-scheduler-minikube" [dfbd06d2-c864-43d7-b156-3f8e0fab23d4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0630 17:32:17.476277  216326 system_pods.go:61] "storage-provisioner" [30fb41d6-b6ea-4e34-9b6a-802d389aace9] Running
I0630 17:32:17.476303  216326 system_pods.go:74] duration metric: took 26.196508ms to wait for pod list to return data ...
I0630 17:32:17.476338  216326 kubeadm.go:576] duration metric: took 6.18064818s to wait for: map[apiserver:true system_pods:true]
I0630 17:32:17.476379  216326 node_conditions.go:102] verifying NodePressure condition ...
I0630 17:32:17.487971  216326 node_conditions.go:122] node storage ephemeral capacity is 959786032Ki
I0630 17:32:17.488030  216326 node_conditions.go:123] node cpu capacity is 8
I0630 17:32:17.488052  216326 node_conditions.go:105] duration metric: took 11.661467ms to run NodePressure ...
I0630 17:32:17.488090  216326 start.go:240] waiting for startup goroutines ...
I0630 17:32:17.488116  216326 start.go:245] waiting for cluster config update ...
I0630 17:32:17.488151  216326 start.go:254] writing updated cluster config ...
I0630 17:32:17.489001  216326 ssh_runner.go:195] Run: rm -f paused
I0630 17:32:17.620896  216326 start.go:600] kubectl: 1.30.2, cluster: 1.25.3 (minor skew: 5)
I0630 17:32:17.622969  216326 out.go:177] 
W0630 17:32:17.625036  216326 out.go:239] ❗  /usr/local/bin/kubectl is version 1.30.2, which may have incompatibilities with Kubernetes 1.25.3.
I0630 17:32:17.626720  216326 out.go:177]     ▪ Want kubectl v1.25.3? Try 'minikube kubectl -- get pods -A'
I0630 17:32:17.629969  216326 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
-- Logs begin at Sun 2024-06-30 16:32:02 UTC, end at Sun 2024-06-30 16:37:15 UTC. --
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000da4400 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc0005e7640 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000ac6380 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc0006285c0 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000d24180 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000d24f80 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000629440 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc0005e7d80 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000d25600 linux}"
Jun 30 16:36:52 minikube cri-dockerd[1144]: time="2024-06-30T16:36:52Z" level=error msg="ContainerStats resp: {0xc000049180 linux}"
Jun 30 16:36:53 minikube cri-dockerd[1144]: time="2024-06-30T16:36:53Z" level=error msg="ContainerStats resp: {0xc000570f40 linux}"
Jun 30 16:36:53 minikube cri-dockerd[1144]: time="2024-06-30T16:36:53Z" level=error msg="ContainerStats resp: {0xc00095dac0 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000ac6b80 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000d91fc0 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc0005e6400 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000ac7e40 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc0005e68c0 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000024f00 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000d24080 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000d24700 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000d24bc0 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc0000245c0 linux}"
Jun 30 16:36:54 minikube cri-dockerd[1144]: time="2024-06-30T16:36:54Z" level=error msg="ContainerStats resp: {0xc000024a00 linux}"
Jun 30 16:36:56 minikube cri-dockerd[1144]: time="2024-06-30T16:36:56Z" level=info msg="Using CNI configuration file /etc/cni/net.d/1-k8s.conflist"
Jun 30 16:37:02 minikube cri-dockerd[1144]: time="2024-06-30T16:37:02Z" level=info msg="Using CNI configuration file /etc/cni/net.d/1-k8s.conflist"
Jun 30 16:37:02 minikube cri-dockerd[1144]: time="2024-06-30T16:37:02Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: key is not found"
Jun 30 16:37:02 minikube cri-dockerd[1144]: time="2024-06-30T16:37:02Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: key is not found"
Jun 30 16:37:02 minikube cri-dockerd[1144]: time="2024-06-30T16:37:02Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: key is not found"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc0005e6480 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc0005e6c00 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000c89280 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000c893c0 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc0005e6140 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000024ec0 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000d90980 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000025000 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000d90b00 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc0005e7800 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000d91380 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc00095c780 linux}"
Jun 30 16:37:05 minikube cri-dockerd[1144]: time="2024-06-30T16:37:05Z" level=error msg="ContainerStats resp: {0xc000c88040 linux}"
Jun 30 16:37:06 minikube cri-dockerd[1144]: time="2024-06-30T16:37:06Z" level=error msg="ContainerStats resp: {0xc000d91780 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=info msg="Using CNI configuration file /etc/cni/net.d/1-k8s.conflist"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc000d246c0 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc000d90040 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc00095c500 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc00095ca40 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc0005e6e40 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc0005e6f00 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc000d91380 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc0005e7600 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc0008932c0 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc00095d640 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc00095db40 linux}"
Jun 30 16:37:07 minikube cri-dockerd[1144]: time="2024-06-30T16:37:07Z" level=error msg="ContainerStats resp: {0xc00095df40 linux}"
Jun 30 16:37:12 minikube cri-dockerd[1144]: time="2024-06-30T16:37:12Z" level=info msg="Using CNI configuration file /etc/cni/net.d/1-k8s.conflist"
Jun 30 16:37:13 minikube cri-dockerd[1144]: time="2024-06-30T16:37:13Z" level=info msg="Pulling image jenkins/jenkins:latest: fea1432adf09: Downloading [=>                                                 ]  1.526MB/49.58MB"
Jun 30 16:37:14 minikube cri-dockerd[1144]: time="2024-06-30T16:37:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: key is not found"
Jun 30 16:37:14 minikube cri-dockerd[1144]: time="2024-06-30T16:37:14Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: key is not found"
Jun 30 16:37:14 minikube cri-dockerd[1144]: time="2024-06-30T16:37:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: key is not found"


==> container status <==
CONTAINER           IMAGE                                                                                     CREATED             STATE               NAME                        ATTEMPT             POD ID
925d5e8f887e9       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64     41 seconds ago      Running             mongo-express               100                 8b43a5f1f9b14
3dee75b953162       obieh/node.test@sha256:fd0ae892953e53a7f7cbacb1c222be3f89758de1f4e4201e82931db887844ae5   2 minutes ago       Running             nodeserver                  6                   d2bb85c51c405
138d042b4ba98       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64     2 minutes ago       Exited              mongo-express               99                  8b43a5f1f9b14
7f51675f42abf       obieh/node.test@sha256:fd0ae892953e53a7f7cbacb1c222be3f89758de1f4e4201e82931db887844ae5   2 minutes ago       Running             nodeserver                  6                   64b6f7342ec9d
ab7d12667d985       nginx@sha256:9c367186df9a6b18c6735357b8eb7f407347e84aea09beb184961cb83543d46e             4 minutes ago       Running             nginx                       9                   5656c285239b5
1bd0619bbc9a7       07655ddf2eebe                                                                             4 minutes ago       Running             kubernetes-dashboard        8                   706a0441a3d0b
52de638f12d4c       6e38f40d628db                                                                             4 minutes ago       Running             storage-provisioner         18                  3703611d5cde8
ceef7a53b9fe4       5185b96f0becf                                                                             4 minutes ago       Running             coredns                     15                  01c1a673eddea
376dd803c60ca       115053965e86b                                                                             4 minutes ago       Running             dashboard-metrics-scraper   6                   948c501e162ca
dfbc3718c84c8       beaaf00edd38a                                                                             4 minutes ago       Running             kube-proxy                  11                  836f5c0c3ddd8
a566a4352e9fa       0346dbd74bcb9                                                                             5 minutes ago       Running             kube-apiserver              10                  f7d64a6512df0
ce4ff4ffd8d0e       6d23ec0e8b87e                                                                             5 minutes ago       Running             kube-scheduler              10                  cd4e85a15f991
762aa9770c2cb       a8a176a5d5d69                                                                             5 minutes ago       Running             etcd                        10                  67c3ef4bf63aa
c9644a01bde4d       6039992312758                                                                             5 minutes ago       Running             kube-controller-manager     11                  3e8ae665cbe07
1764300c41981       6e38f40d628db                                                                             2 hours ago         Exited              storage-provisioner         17                  e05fa0af380ed
6793a8b03155b       obieh/node.test@sha256:fd0ae892953e53a7f7cbacb1c222be3f89758de1f4e4201e82931db887844ae5   2 hours ago         Exited              nodeserver                  5                   74a35b473cc68
039510587f186       obieh/node.test@sha256:fd0ae892953e53a7f7cbacb1c222be3f89758de1f4e4201e82931db887844ae5   2 hours ago         Exited              nodeserver                  5                   cf9b214595818
a38d948bf6c60       nginx@sha256:9c367186df9a6b18c6735357b8eb7f407347e84aea09beb184961cb83543d46e             2 hours ago         Exited              nginx                       8                   79c162a951f22
991250761f0dc       115053965e86b                                                                             2 hours ago         Exited              dashboard-metrics-scraper   5                   f5d041c6c67a7
671b7df479e56       07655ddf2eebe                                                                             2 hours ago         Exited              kubernetes-dashboard        7                   daddf0faf2f95
36b65229421e8       beaaf00edd38a                                                                             2 hours ago         Exited              kube-proxy                  10                  6bc1d2585b310
b5b27e20b4f69       5185b96f0becf                                                                             2 hours ago         Exited              coredns                     14                  46dcb2531a4a8
6cc945aedb30c       0346dbd74bcb9                                                                             2 hours ago         Exited              kube-apiserver              9                   ebc302fc8f0de
aa5182c2e151d       6039992312758                                                                             2 hours ago         Exited              kube-controller-manager     10                  2d66be81c9bf7
86f33074d1960       a8a176a5d5d69                                                                             2 hours ago         Exited              etcd                        9                   ad6a703297395
c67ebf09a4b74       6d23ec0e8b87e                                                                             2 hours ago         Exited              kube-scheduler              9                   893a5dbf49c3f


==> coredns [b5b27e20b4f6] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [ceef7a53b9fe] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_26T17_11_21_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 05 Nov 2022 12:37:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 30 Jun 2024 16:37:11 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 30 Jun 2024 16:32:17 +0000   Sat, 05 Nov 2022 12:37:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 30 Jun 2024 16:32:17 +0000   Sat, 05 Nov 2022 12:37:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 30 Jun 2024 16:32:17 +0000   Sat, 05 Nov 2022 12:37:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 30 Jun 2024 16:32:17 +0000   Sat, 05 Nov 2022 12:37:56 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  959786032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32661680Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  959786032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32661680Ki
  pods:               110
System Info:
  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9
  System UUID:                1b5a1a7f-8d95-49a5-8833-210f3beb497d
  Boot ID:                    3b170707-b340-4634-a97f-2c6901de9569
  Kernel Version:             6.5.0-41-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (15 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-5bf4b56f47-rwnms                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         469d
  default                     mongodb-deployment-844789cd64-m8ldc           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         469d
  default                     ngin-depl-5cf96595c9-khckw                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         469d
  default                     nodeapp-deployment-6b7b47f696-lc8p5           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  default                     nodeapp-deployment-6b7b47f696-z2m5k           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  dev                         jenkinsapp                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         39m
  kube-system                 coredns-565d847f94-22v8l                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     603d
  kube-system                 etcd-minikube                                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         603d
  kube-system                 kube-apiserver-minikube                       250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         603d
  kube-system                 kube-controller-manager-minikube              200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         603d
  kube-system                 kube-proxy-z52gj                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         603d
  kube-system                 kube-scheduler-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         603d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         603d
  kubernetes-dashboard        dashboard-metrics-scraper-5f5c79dd8f-fp9m5    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  kubernetes-dashboard        kubernetes-dashboard-f87d45d87-r845g          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (0%!)(MISSING)  170Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 4m56s                kube-proxy       
  Normal  Starting                 5m4s                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  5m4s (x9 over 5m4s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m4s (x7 over 5m4s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m4s (x7 over 5m4s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m4s                 kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           4m47s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000009]  ? partition_sched_domains_locked+0x41d/0x620
[  +0.000011]  ? __pfx_supdrvGipMpEvent+0x10/0x10 [vboxdrv]
[  +0.000097]  rtMpNotificationDoCallbacks+0xde/0x110 [vboxdrv]
[  +0.000107]  ? __pfx_rtR0MpNotificationLinuxOffline+0x10/0x10 [vboxdrv]
[  +0.000115]  rtR0MpNotificationLinuxOffline+0x1a/0x30 [vboxdrv]
[  +0.000109]  cpuhp_invoke_callback+0x345/0x530
[  +0.000009]  ? __schedule+0x2d3/0x750
[  +0.000006]  cpuhp_thread_fun+0x13c/0x1f0
[  +0.000005]  ? __pfx_smpboot_thread_fn+0x10/0x10
[  +0.000007]  smpboot_thread_fn+0xe0/0x1e0
[  +0.000007]  kthread+0xef/0x120
[  +0.000009]  ? __pfx_kthread+0x10/0x10
[  +0.000008]  ret_from_fork+0x44/0x70
[  +0.000009]  ? __pfx_kthread+0x10/0x10
[  +0.000008]  ret_from_fork_asm+0x1b/0x30
[  +0.000008]  </TASK>
[  +0.000001] ================================================================================
[  +0.045435] ================================================================================
[  +0.000003] UBSAN: array-index-out-of-bounds in /var/lib/dkms/virtualbox/6.1.50/build/vboxdrv/SUPDrvGip.c:4402:64
[  +0.000002] index 2 is out of range for type 'SUPGIPCPU [1]'
[  +0.000002] CPU: 1 PID: 2021 Comm: iprt-VBoxTscThr Tainted: G           OE      6.5.0-41-generic #41~22.04.2-Ubuntu
[  +0.000002] Hardware name: HP HP EliteBook 840 G6/8549, BIOS R70 Ver. 01.02.01 08/26/2019
[  +0.000001] Call Trace:
[  +0.000001]  <TASK>
[  +0.000002]  dump_stack_lvl+0x48/0x70
[  +0.000008]  dump_stack+0x10/0x20
[  +0.000002]  __ubsan_handle_out_of_bounds+0xc6/0x110
[  +0.000002]  supdrvTscDeltaThread+0x74b/0x7c0 [vboxdrv]
[  +0.000032]  ? __pfx_rtThreadNativeMain+0x10/0x10 [vboxdrv]
[  +0.000028]  rtThreadMain+0x34/0x80 [vboxdrv]
[  +0.000025]  rtThreadNativeMain+0x1b/0x30 [vboxdrv]
[  +0.000025]  kthread+0xef/0x120
[  +0.000003]  ? __pfx_kthread+0x10/0x10
[  +0.000002]  ret_from_fork+0x44/0x70
[  +0.000003]  ? __pfx_kthread+0x10/0x10
[  +0.000002]  ret_from_fork_asm+0x1b/0x30
[  +0.000002]  </TASK>
[  +0.000001] ================================================================================
[  +1.786371] ata3.00: supports DRM functions and may not be fully accessible
[  +0.004291] ata3.00: supports DRM functions and may not be fully accessible
[  +1.009798] done.
[  +0.157734] thermal thermal_zone12: failed to read out thermal zone (-61)
[  +1.799251] Bluetooth: hci0: Malformed MSFT vendor event: 0x02
[  +0.005002] Bluetooth: hci0: HCI LE Coded PHY feature bit is set, but its usage is not supported.
[Jun30 14:10] usb usb2-port2: unable to enumerate USB device
[ +42.644684] sd 3:0:0:0: [sdb] Optimal transfer size 33553920 bytes not a multiple of preferred minimum block size (4096 bytes)
[Jun30 14:11] sd 3:0:0:0: [sdb] Optimal transfer size 33553920 bytes not a multiple of preferred minimum block size (4096 bytes)
[Jun30 14:12] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[ +41.328068] kauditd_printk_skb: 2 callbacks suppressed
[Jun30 14:16] workqueue: delayed_fput hogged CPU for >10666us 8 times, consider switching to WQ_UNBOUND
[Jun30 15:16] workqueue: delayed_fput hogged CPU for >10666us 16 times, consider switching to WQ_UNBOUND
[Jun30 15:33] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.018466] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.000079] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.000031] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.000024] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.056422] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[  +0.000064] iwlwifi 0000:3a:00.0: Unhandled alg: 0x703
[Jun30 15:46] workqueue: delayed_fput hogged CPU for >10666us 32 times, consider switching to WQ_UNBOUND
[Jun30 16:32] show_signal_msg: 12 callbacks suppressed


==> etcd [762aa9770c2c] <==
{"level":"info","ts":"2024-06-30T16:32:12.821Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-06-30T16:32:12.821Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2024-06-30T16:32:12.821Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-30T16:32:12.821Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-30T16:32:12.822Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-30T16:32:12.823Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.4","git-sha":"08407ff76","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-30T16:32:12.826Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.500857ms"}
{"level":"info","ts":"2024-06-30T16:32:13.402Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":480048,"snapshot-size":"16 kB"}
{"level":"info","ts":"2024-06-30T16:32:13.402Z","caller":"etcdserver/server.go:521","msg":"recovered v3 backend from snapshot","backend-size-bytes":3809280,"backend-size":"3.8 MB","backend-size-in-use-bytes":1499136,"backend-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-30T16:32:13.517Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":482294}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 26"}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 26, commit: 482294, applied: 480048, lastindex: 482294, lastterm: 26]"}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-30T16:32:13.518Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-06-30T16:32:13.520Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-30T16:32:13.521Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":374261}
{"level":"info","ts":"2024-06-30T16:32:13.523Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":374679}
{"level":"info","ts":"2024-06-30T16:32:13.524Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-30T16:32:13.527Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-06-30T16:32:13.527Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-30T16:32:13.527Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.4","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-30T16:32:13.527Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-06-30T16:32:13.529Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-30T16:32:13.529Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-30T16:32:13.529Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-30T16:32:13.529Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-30T16:32:13.529Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 26"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 26"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 26"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 27"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 27"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 27"}
{"level":"info","ts":"2024-06-30T16:32:14.519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 27"}
{"level":"info","ts":"2024-06-30T16:32:14.524Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-30T16:32:14.525Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-30T16:32:14.525Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-30T16:32:14.525Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-30T16:32:14.525Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-30T16:32:14.529Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-30T16:32:14.529Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}


==> etcd [86f33074d196] <==
{"level":"info","ts":"2024-06-30T14:37:07.595Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-30T14:37:07.596Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-30T14:37:07.596Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-30T14:37:07.596Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-30T14:37:07.600Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-30T14:37:07.600Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-30T14:47:07.631Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":369756}
{"level":"info","ts":"2024-06-30T14:47:07.652Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":369756,"took":"19.777136ms"}
{"level":"info","ts":"2024-06-30T14:52:07.641Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":370000}
{"level":"info","ts":"2024-06-30T14:52:07.643Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":370000,"took":"1.744831ms"}
{"level":"info","ts":"2024-06-30T14:57:07.652Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":370227}
{"level":"info","ts":"2024-06-30T14:57:07.656Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":370227,"took":"3.068738ms"}
{"level":"info","ts":"2024-06-30T15:02:07.662Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":370445}
{"level":"info","ts":"2024-06-30T15:02:07.664Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":370445,"took":"1.560793ms"}
{"level":"info","ts":"2024-06-30T15:07:07.670Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":370668}
{"level":"info","ts":"2024-06-30T15:07:07.672Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":370668,"took":"1.698905ms"}
{"level":"info","ts":"2024-06-30T15:12:07.678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":370892}
{"level":"info","ts":"2024-06-30T15:12:07.681Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":370892,"took":"1.692729ms"}
{"level":"info","ts":"2024-06-30T15:17:07.686Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":371109}
{"level":"info","ts":"2024-06-30T15:17:07.688Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":371109,"took":"1.458522ms"}
{"level":"info","ts":"2024-06-30T15:22:07.694Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":371327}
{"level":"info","ts":"2024-06-30T15:22:07.696Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":371327,"took":"1.345299ms"}
{"level":"info","ts":"2024-06-30T15:27:07.710Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":371551}
{"level":"info","ts":"2024-06-30T15:27:07.713Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":371551,"took":"2.469403ms"}
{"level":"info","ts":"2024-06-30T15:32:07.718Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":371773}
{"level":"info","ts":"2024-06-30T15:32:07.720Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":371773,"took":"1.377854ms"}
{"level":"info","ts":"2024-06-30T15:37:07.734Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":371992}
{"level":"info","ts":"2024-06-30T15:37:07.736Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":371992,"took":"1.07286ms"}
{"level":"info","ts":"2024-06-30T15:42:07.742Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":372210}
{"level":"info","ts":"2024-06-30T15:42:07.744Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":372210,"took":"1.315515ms"}
{"level":"info","ts":"2024-06-30T15:47:07.753Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":372440}
{"level":"info","ts":"2024-06-30T15:47:07.756Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":372440,"took":"2.308642ms"}
{"level":"info","ts":"2024-06-30T15:52:07.762Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":372664}
{"level":"info","ts":"2024-06-30T15:52:07.765Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":372664,"took":"1.617892ms"}
{"level":"info","ts":"2024-06-30T15:52:57.317Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":480048,"local-member-snapshot-index":470047,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-06-30T15:52:57.325Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":480048}
{"level":"info","ts":"2024-06-30T15:52:57.325Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":475048}
{"level":"info","ts":"2024-06-30T15:53:07.120Z","caller":"fileutil/purge.go:77","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000010-0000000000068fdb.snap"}
{"level":"info","ts":"2024-06-30T15:57:07.773Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":372886}
{"level":"info","ts":"2024-06-30T15:57:07.778Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":372886,"took":"4.123426ms"}
{"level":"info","ts":"2024-06-30T16:02:07.783Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":373105}
{"level":"info","ts":"2024-06-30T16:02:07.785Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":373105,"took":"1.793239ms"}
{"level":"info","ts":"2024-06-30T16:07:07.792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":373349}
{"level":"info","ts":"2024-06-30T16:07:07.795Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":373349,"took":"1.651052ms"}
{"level":"info","ts":"2024-06-30T16:12:07.801Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":373592}
{"level":"info","ts":"2024-06-30T16:12:07.803Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":373592,"took":"1.107595ms"}
{"level":"info","ts":"2024-06-30T16:17:07.810Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":373807}
{"level":"info","ts":"2024-06-30T16:17:07.812Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":373807,"took":"1.439861ms"}
{"level":"info","ts":"2024-06-30T16:22:07.819Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":374034}
{"level":"info","ts":"2024-06-30T16:22:07.821Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":374034,"took":"1.705054ms"}
{"level":"info","ts":"2024-06-30T16:27:07.825Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":374261}
{"level":"info","ts":"2024-06-30T16:27:07.827Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":374261,"took":"1.400077ms"}
{"level":"info","ts":"2024-06-30T16:31:37.288Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-06-30T16:31:37.289Z","caller":"embed/etcd.go:368","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2024/06/30 16:31:37 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2024/06/30 16:31:37 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2024-06-30T16:31:37.310Z","caller":"etcdserver/server.go:1453","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-30T16:31:37.317Z","caller":"embed/etcd.go:563","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-30T16:31:37.319Z","caller":"embed/etcd.go:568","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-30T16:31:37.319Z","caller":"embed/etcd.go:370","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 16:37:15 up  3:45,  0 users,  load average: 4.09, 3.56, 3.23
Linux minikube 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"


==> kube-apiserver [6cc945aedb30] <==
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.302828       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.302923       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.303016       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.303081       1 logging.go:59] [core] [Channel #13 SubChannel #14] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.303136       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.303161       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0630 16:31:37.303241       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [a566a4352e9f] <==
W0630 16:32:15.040648       1 genericapiserver.go:656] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0630 16:32:15.040668       1 genericapiserver.go:656] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.044103       1 genericapiserver.go:656] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0630 16:32:15.044112       1 genericapiserver.go:656] Skipping API node.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.044167       1 genericapiserver.go:656] Skipping API policy/v1beta1 because it has no resources.
W0630 16:32:15.047421       1 genericapiserver.go:656] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0630 16:32:15.047430       1 genericapiserver.go:656] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.048551       1 genericapiserver.go:656] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0630 16:32:15.048557       1 genericapiserver.go:656] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.051492       1 genericapiserver.go:656] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.054186       1 genericapiserver.go:656] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0630 16:32:15.056866       1 genericapiserver.go:656] Skipping API apps/v1beta2 because it has no resources.
W0630 16:32:15.056874       1 genericapiserver.go:656] Skipping API apps/v1beta1 because it has no resources.
W0630 16:32:15.058046       1 genericapiserver.go:656] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0630 16:32:15.059037       1 genericapiserver.go:656] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0630 16:32:15.059583       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0630 16:32:15.059589       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0630 16:32:15.072888       1 genericapiserver.go:656] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0630 16:32:15.921291       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0630 16:32:15.921354       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0630 16:32:15.921361       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0630 16:32:15.921539       1 secure_serving.go:210] Serving securely on [::]:8443
I0630 16:32:15.921583       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0630 16:32:15.921589       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0630 16:32:15.921630       1 autoregister_controller.go:141] Starting autoregister controller
I0630 16:32:15.921636       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0630 16:32:15.921644       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0630 16:32:15.921648       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0630 16:32:15.921663       1 controller.go:83] Starting OpenAPI AggregationController
I0630 16:32:15.921671       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0630 16:32:15.921637       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0630 16:32:15.921676       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I0630 16:32:15.921845       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0630 16:32:15.921852       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I0630 16:32:15.921883       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0630 16:32:15.938074       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I0630 16:32:15.938228       1 available_controller.go:491] Starting AvailableConditionController
I0630 16:32:15.938268       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0630 16:32:15.938777       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0630 16:32:15.938100       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0630 16:32:15.942966       1 establishing_controller.go:76] Starting EstablishingController
I0630 16:32:15.942995       1 controller.go:85] Starting OpenAPI controller
I0630 16:32:15.943032       1 controller.go:85] Starting OpenAPI V3 controller
I0630 16:32:15.943044       1 naming_controller.go:291] Starting NamingConditionController
I0630 16:32:15.943059       1 crd_finalizer.go:266] Starting CRDFinalizer
I0630 16:32:15.943078       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0630 16:32:15.943106       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
E0630 16:32:15.955708       1 controller.go:159] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0630 16:32:15.970235       1 shared_informer.go:262] Caches are synced for node_authorizer
I0630 16:32:16.021691       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0630 16:32:16.021695       1 cache.go:39] Caches are synced for autoregister controller
I0630 16:32:16.021833       1 shared_informer.go:262] Caches are synced for crd-autoregister
I0630 16:32:16.021884       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I0630 16:32:16.039148       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0630 16:32:16.039152       1 apf_controller.go:305] Running API Priority and Fairness config worker
I0630 16:32:16.050101       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I0630 16:32:16.761603       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0630 16:32:16.926783       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0630 16:32:28.952745       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0630 16:32:28.976476       1 controller.go:616] quota admission added evaluator for: endpoints


==> kube-controller-manager [aa5182c2e151] <==
W0630 14:37:21.380790       1 core.go:232] configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes.
W0630 14:37:21.380811       1 controllermanager.go:581] Skipping "route"
I0630 14:37:21.381108       1 job_controller.go:196] Starting job controller
I0630 14:37:21.381136       1 shared_informer.go:255] Waiting for caches to sync for job
I0630 14:37:21.383764       1 controllermanager.go:603] Started "ephemeral-volume"
I0630 14:37:21.383837       1 controller.go:169] Starting ephemeral volume controller
I0630 14:37:21.383858       1 shared_informer.go:255] Waiting for caches to sync for ephemeral
I0630 14:37:21.386221       1 controllermanager.go:603] Started "replicationcontroller"
I0630 14:37:21.386381       1 replica_set.go:205] Starting replicationcontroller controller
I0630 14:37:21.386395       1 shared_informer.go:255] Waiting for caches to sync for ReplicationController
I0630 14:37:21.398934       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W0630 14:37:21.407019       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0630 14:37:21.407958       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0630 14:37:21.409127       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0630 14:37:21.411016       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0630 14:37:21.417024       1 shared_informer.go:262] Caches are synced for deployment
I0630 14:37:21.422081       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0630 14:37:21.427650       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0630 14:37:21.439503       1 shared_informer.go:262] Caches are synced for service account
I0630 14:37:21.441737       1 shared_informer.go:262] Caches are synced for node
I0630 14:37:21.441859       1 range_allocator.go:166] Starting range CIDR allocator
I0630 14:37:21.441875       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0630 14:37:21.441908       1 shared_informer.go:262] Caches are synced for cidrallocator
I0630 14:37:21.446221       1 shared_informer.go:262] Caches are synced for disruption
I0630 14:37:21.452515       1 shared_informer.go:262] Caches are synced for TTL after finished
I0630 14:37:21.453723       1 shared_informer.go:262] Caches are synced for PV protection
I0630 14:37:21.454932       1 shared_informer.go:262] Caches are synced for daemon sets
I0630 14:37:21.457215       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0630 14:37:21.465108       1 shared_informer.go:262] Caches are synced for GC
I0630 14:37:21.471087       1 shared_informer.go:262] Caches are synced for cronjob
I0630 14:37:21.482014       1 shared_informer.go:262] Caches are synced for job
I0630 14:37:21.485181       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0630 14:37:21.486096       1 shared_informer.go:262] Caches are synced for namespace
I0630 14:37:21.486716       1 shared_informer.go:262] Caches are synced for ReplicationController
I0630 14:37:21.491649       1 shared_informer.go:262] Caches are synced for endpoint
I0630 14:37:21.491800       1 shared_informer.go:262] Caches are synced for TTL
I0630 14:37:21.498686       1 shared_informer.go:262] Caches are synced for crt configmap
I0630 14:37:21.505469       1 shared_informer.go:262] Caches are synced for HPA
I0630 14:37:21.505892       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0630 14:37:21.507118       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0630 14:37:21.547890       1 shared_informer.go:262] Caches are synced for taint
I0630 14:37:21.547993       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
I0630 14:37:21.548022       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I0630 14:37:21.548089       1 taint_manager.go:209] "Sending events to api server"
W0630 14:37:21.548104       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0630 14:37:21.548197       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0630 14:37:21.548225       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I0630 14:37:21.550302       1 shared_informer.go:262] Caches are synced for persistent volume
I0630 14:37:21.580084       1 shared_informer.go:262] Caches are synced for attach detach
I0630 14:37:21.584496       1 shared_informer.go:262] Caches are synced for ephemeral
I0630 14:37:21.592684       1 shared_informer.go:262] Caches are synced for PVC protection
I0630 14:37:21.610339       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0630 14:37:21.623467       1 shared_informer.go:262] Caches are synced for stateful set
I0630 14:37:21.635415       1 shared_informer.go:262] Caches are synced for expand
I0630 14:37:21.661429       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0630 14:37:21.677980       1 shared_informer.go:262] Caches are synced for resource quota
I0630 14:37:21.699414       1 shared_informer.go:262] Caches are synced for resource quota
I0630 14:37:22.023058       1 shared_informer.go:262] Caches are synced for garbage collector
I0630 14:37:22.096199       1 shared_informer.go:262] Caches are synced for garbage collector
I0630 14:37:22.096259       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage


==> kube-controller-manager [c9644a01bde4] <==
I0630 16:32:28.875075       1 range_allocator.go:110] No Secondary Service CIDR provided. Skipping filtering out secondary service addresses.
I0630 16:32:28.875189       1 controllermanager.go:603] Started "nodeipam"
I0630 16:32:28.875308       1 node_ipam_controller.go:154] Starting ipam controller
I0630 16:32:28.875353       1 shared_informer.go:255] Waiting for caches to sync for node
I0630 16:32:28.881148       1 controllermanager.go:603] Started "ephemeral-volume"
I0630 16:32:28.881267       1 controller.go:169] Starting ephemeral volume controller
I0630 16:32:28.881292       1 shared_informer.go:255] Waiting for caches to sync for ephemeral
I0630 16:32:28.883902       1 controllermanager.go:603] Started "daemonset"
I0630 16:32:28.884114       1 daemon_controller.go:291] Starting daemon sets controller
I0630 16:32:28.884127       1 shared_informer.go:255] Waiting for caches to sync for daemon sets
I0630 16:32:28.890777       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W0630 16:32:28.896339       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0630 16:32:28.906378       1 shared_informer.go:262] Caches are synced for service account
I0630 16:32:28.910644       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0630 16:32:28.927369       1 shared_informer.go:262] Caches are synced for GC
I0630 16:32:28.928469       1 shared_informer.go:262] Caches are synced for ReplicationController
I0630 16:32:28.928535       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0630 16:32:28.936967       1 shared_informer.go:262] Caches are synced for TTL
I0630 16:32:28.939439       1 shared_informer.go:262] Caches are synced for taint
I0630 16:32:28.939511       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
I0630 16:32:28.939515       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
W0630 16:32:28.939550       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0630 16:32:28.939579       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I0630 16:32:28.939550       1 taint_manager.go:209] "Sending events to api server"
I0630 16:32:28.939658       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0630 16:32:28.942285       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0630 16:32:28.943448       1 shared_informer.go:262] Caches are synced for TTL after finished
I0630 16:32:28.945646       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0630 16:32:28.946874       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0630 16:32:28.957372       1 shared_informer.go:262] Caches are synced for namespace
I0630 16:32:28.965706       1 shared_informer.go:262] Caches are synced for deployment
I0630 16:32:28.967083       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0630 16:32:28.968235       1 shared_informer.go:262] Caches are synced for crt configmap
I0630 16:32:28.970436       1 shared_informer.go:262] Caches are synced for endpoint
I0630 16:32:28.971518       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0630 16:32:28.971781       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0630 16:32:28.972696       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0630 16:32:28.972701       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0630 16:32:28.975464       1 shared_informer.go:262] Caches are synced for node
I0630 16:32:28.975478       1 range_allocator.go:166] Starting range CIDR allocator
I0630 16:32:28.975482       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0630 16:32:28.975491       1 shared_informer.go:262] Caches are synced for cidrallocator
I0630 16:32:28.976203       1 shared_informer.go:262] Caches are synced for PVC protection
I0630 16:32:28.977612       1 shared_informer.go:262] Caches are synced for PV protection
I0630 16:32:28.977637       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0630 16:32:28.978012       1 shared_informer.go:262] Caches are synced for job
I0630 16:32:28.978367       1 shared_informer.go:262] Caches are synced for persistent volume
I0630 16:32:28.978418       1 shared_informer.go:262] Caches are synced for stateful set
I0630 16:32:28.981552       1 shared_informer.go:262] Caches are synced for ephemeral
I0630 16:32:28.982646       1 shared_informer.go:262] Caches are synced for disruption
I0630 16:32:28.982648       1 shared_informer.go:262] Caches are synced for HPA
I0630 16:32:28.984883       1 shared_informer.go:262] Caches are synced for daemon sets
I0630 16:32:28.986876       1 shared_informer.go:262] Caches are synced for expand
I0630 16:32:28.990840       1 shared_informer.go:262] Caches are synced for cronjob
I0630 16:32:28.995788       1 shared_informer.go:262] Caches are synced for attach detach
I0630 16:32:29.135585       1 shared_informer.go:262] Caches are synced for resource quota
I0630 16:32:29.191582       1 shared_informer.go:262] Caches are synced for resource quota
I0630 16:32:29.511995       1 shared_informer.go:262] Caches are synced for garbage collector
I0630 16:32:29.561527       1 shared_informer.go:262] Caches are synced for garbage collector
I0630 16:32:29.561590       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage


==> kube-proxy [36b65229421e] <==
I0630 14:37:12.449106       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0630 14:37:12.449158       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0630 14:37:12.449179       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0630 14:37:12.472597       1 server_others.go:206] "Using iptables Proxier"
I0630 14:37:12.472634       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0630 14:37:12.472655       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0630 14:37:12.472672       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0630 14:37:12.472991       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0630 14:37:12.473123       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0630 14:37:12.473285       1 server.go:661] "Version info" version="v1.25.3"
I0630 14:37:12.473298       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0630 14:37:12.475825       1 config.go:317] "Starting service config controller"
I0630 14:37:12.475825       1 config.go:226] "Starting endpoint slice config controller"
I0630 14:37:12.475839       1 shared_informer.go:255] Waiting for caches to sync for service config
I0630 14:37:12.475841       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0630 14:37:12.476220       1 config.go:444] "Starting node config controller"
I0630 14:37:12.476229       1 shared_informer.go:255] Waiting for caches to sync for node config
I0630 14:37:12.576802       1 shared_informer.go:262] Caches are synced for node config
I0630 14:37:12.577022       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0630 14:37:12.577132       1 shared_informer.go:262] Caches are synced for service config


==> kube-proxy [dfbc3718c84c] <==
I0630 16:32:19.124667       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0630 16:32:19.124740       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0630 16:32:19.124771       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0630 16:32:19.147338       1 server_others.go:206] "Using iptables Proxier"
I0630 16:32:19.147366       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0630 16:32:19.147377       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0630 16:32:19.147415       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0630 16:32:19.147455       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0630 16:32:19.147667       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0630 16:32:19.147963       1 server.go:661] "Version info" version="v1.25.3"
I0630 16:32:19.147988       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0630 16:32:19.149482       1 config.go:317] "Starting service config controller"
I0630 16:32:19.149515       1 shared_informer.go:255] Waiting for caches to sync for service config
I0630 16:32:19.149553       1 config.go:226] "Starting endpoint slice config controller"
I0630 16:32:19.149570       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0630 16:32:19.149669       1 config.go:444] "Starting node config controller"
I0630 16:32:19.149686       1 shared_informer.go:255] Waiting for caches to sync for node config
I0630 16:32:19.250047       1 shared_informer.go:262] Caches are synced for node config
I0630 16:32:19.250073       1 shared_informer.go:262] Caches are synced for service config
I0630 16:32:19.250075       1 shared_informer.go:262] Caches are synced for endpoint slice config


==> kube-scheduler [c67ebf09a4b7] <==
I0630 14:37:06.579690       1 serving.go:348] Generated self-signed cert in-memory
W0630 14:37:09.104213       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0630 14:37:09.104952       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0630 14:37:09.105012       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0630 14:37:09.105034       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0630 14:37:09.117074       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I0630 14:37:09.117088       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0630 14:37:09.118686       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0630 14:37:09.118734       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0630 14:37:09.118774       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0630 14:37:09.119637       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0630 14:37:09.220328       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0630 16:31:37.289397       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0630 16:31:37.289434       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0630 16:31:37.290369       1 scheduling_queue.go:963] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
I0630 16:31:37.290445       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0630 16:31:37.290653       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [ce4ff4ffd8d0] <==
I0630 16:32:13.341292       1 serving.go:348] Generated self-signed cert in-memory
W0630 16:32:15.940312       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0630 16:32:15.940368       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0630 16:32:15.940410       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0630 16:32:15.940496       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0630 16:32:15.962777       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I0630 16:32:15.962790       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0630 16:32:15.963522       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0630 16:32:15.963538       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0630 16:32:15.963558       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0630 16:32:15.963602       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0630 16:32:16.064350       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
-- Logs begin at Sun 2024-06-30 16:32:02 UTC, end at Sun 2024-06-30 16:37:15 UTC. --
Jun 30 16:34:40 minikube kubelet[1394]: E0630 16:34:40.079881    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:34:40 minikube kubelet[1394]: E0630 16:34:40.079916    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:34:40 minikube kubelet[1394]: E0630 16:34:40.079948    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:34:52 minikube kubelet[1394]: E0630 16:34:52.085905    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:34:52 minikube kubelet[1394]: E0630 16:34:52.086028    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:34:52 minikube kubelet[1394]: E0630 16:34:52.086170    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:34:52 minikube kubelet[1394]: E0630 16:34:52.086330    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:04 minikube kubelet[1394]: E0630 16:35:04.082595    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:35:04 minikube kubelet[1394]: E0630 16:35:04.082678    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:35:04 minikube kubelet[1394]: E0630 16:35:04.082751    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:35:04 minikube kubelet[1394]: E0630 16:35:04.082823    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:16 minikube kubelet[1394]: E0630 16:35:16.085368    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:35:16 minikube kubelet[1394]: E0630 16:35:16.086745    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:35:16 minikube kubelet[1394]: E0630 16:35:16.087223    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:35:16 minikube kubelet[1394]: E0630 16:35:16.088131    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:27 minikube kubelet[1394]: E0630 16:35:27.079700    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:35:27 minikube kubelet[1394]: E0630 16:35:27.079738    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:35:27 minikube kubelet[1394]: E0630 16:35:27.079771    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:35:27 minikube kubelet[1394]: E0630 16:35:27.079803    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:40 minikube kubelet[1394]: E0630 16:35:40.085480    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:35:40 minikube kubelet[1394]: E0630 16:35:40.085588    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:35:40 minikube kubelet[1394]: E0630 16:35:40.085716    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:35:40 minikube kubelet[1394]: E0630 16:35:40.086313    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:53 minikube kubelet[1394]: E0630 16:35:53.080702    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:35:53 minikube kubelet[1394]: E0630 16:35:53.080746    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:35:53 minikube kubelet[1394]: E0630 16:35:53.080790    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:35:53 minikube kubelet[1394]: E0630 16:35:53.080831    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:35:57 minikube kubelet[1394]: I0630 16:35:57.595627    1394 scope.go:115] "RemoveContainer" containerID="6f3dd3f49c64370397396df14980c4a6d093709ae69e6c1fd648d1421a5ca161"
Jun 30 16:35:57 minikube kubelet[1394]: I0630 16:35:57.595883    1394 scope.go:115] "RemoveContainer" containerID="138d042b4ba98f38b03c47887c180c6c52ec0ed5878cd16725152d67b4d5a69f"
Jun 30 16:35:57 minikube kubelet[1394]: E0630 16:35:57.596180    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 10s restarting failed container=mongo-express pod=mongo-express-5bf4b56f47-rwnms_default(0de0bc8a-7ac2-4660-89d1-32fdb69d870a)\"" pod="default/mongo-express-5bf4b56f47-rwnms" podUID=0de0bc8a-7ac2-4660-89d1-32fdb69d870a
Jun 30 16:36:07 minikube kubelet[1394]: E0630 16:36:07.088198    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:36:07 minikube kubelet[1394]: E0630 16:36:07.088371    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:36:07 minikube kubelet[1394]: E0630 16:36:07.088600    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:36:07 minikube kubelet[1394]: E0630 16:36:07.088735    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:36:08 minikube kubelet[1394]: I0630 16:36:08.078286    1394 scope.go:115] "RemoveContainer" containerID="138d042b4ba98f38b03c47887c180c6c52ec0ed5878cd16725152d67b4d5a69f"
Jun 30 16:36:22 minikube kubelet[1394]: E0630 16:36:22.086826    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:36:22 minikube kubelet[1394]: E0630 16:36:22.086978    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:36:22 minikube kubelet[1394]: E0630 16:36:22.087210    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:36:22 minikube kubelet[1394]: E0630 16:36:22.087341    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:36:31 minikube kubelet[1394]: E0630 16:36:31.084496    1394 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="jenkins/jenkins:latest"
Jun 30 16:36:31 minikube kubelet[1394]: E0630 16:36:31.084585    1394 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="jenkins/jenkins:latest"
Jun 30 16:36:31 minikube kubelet[1394]: E0630 16:36:31.084982    1394 kuberuntime_manager.go:862] container &Container{Name:jenkins,Image:jenkins/jenkins:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fvq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod jenkinsapp_dev(599dd1c4-b0ce-4635-aea5-3026b16ee585): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Jun 30 16:36:31 minikube kubelet[1394]: E0630 16:36:31.085078    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jenkins\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="dev/jenkinsapp" podUID=599dd1c4-b0ce-4635-aea5-3026b16ee585
Jun 30 16:36:36 minikube kubelet[1394]: E0630 16:36:36.084722    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:36:36 minikube kubelet[1394]: E0630 16:36:36.084823    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:36:36 minikube kubelet[1394]: E0630 16:36:36.084931    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:36:36 minikube kubelet[1394]: E0630 16:36:36.085047    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:36:46 minikube kubelet[1394]: E0630 16:36:46.082116    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jenkins\" with ImagePullBackOff: \"Back-off pulling image \\\"jenkins/jenkins:latest\\\"\"" pod="dev/jenkinsapp" podUID=599dd1c4-b0ce-4635-aea5-3026b16ee585
Jun 30 16:36:51 minikube kubelet[1394]: E0630 16:36:51.087356    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:36:51 minikube kubelet[1394]: E0630 16:36:51.087499    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:36:51 minikube kubelet[1394]: E0630 16:36:51.087646    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:36:51 minikube kubelet[1394]: E0630 16:36:51.087779    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:37:02 minikube kubelet[1394]: E0630 16:37:02.084915    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:37:02 minikube kubelet[1394]: E0630 16:37:02.085066    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:37:02 minikube kubelet[1394]: E0630 16:37:02.085251    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:37:02 minikube kubelet[1394]: E0630 16:37:02.085395    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54
Jun 30 16:37:14 minikube kubelet[1394]: E0630 16:37:14.085474    1394 remote_runtime.go:269] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"mongodb-deployment-844789cd64-m8ldc_default\" network: could not retrieve port mappings: key is not found" podSandboxID="72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec"
Jun 30 16:37:14 minikube kubelet[1394]: E0630 16:37:14.085582    1394 kuberuntime_manager.go:954] "Failed to stop sandbox" podSandboxID={Type:docker ID:72240b769211f89efa1d3587d458e69915d20938460bd0a05599ccda505437ec}
Jun 30 16:37:14 minikube kubelet[1394]: E0630 16:37:14.085703    1394 kuberuntime_manager.go:695] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\""
Jun 30 16:37:14 minikube kubelet[1394]: E0630 16:37:14.085818    1394 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ca134975-0c87-450c-a62c-af72195f5d54\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"mongodb-deployment-844789cd64-m8ldc_default\\\" network: could not retrieve port mappings: key is not found\"" pod="default/mongodb-deployment-844789cd64-m8ldc" podUID=ca134975-0c87-450c-a62c-af72195f5d54


==> kubernetes-dashboard [1bd0619bbc9a] <==
2024/06/30 16:32:21 Using namespace: kubernetes-dashboard
2024/06/30 16:32:21 Using in-cluster config to connect to apiserver
2024/06/30 16:32:21 Using secret token for csrf signing
2024/06/30 16:32:21 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/06/30 16:32:21 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/06/30 16:32:21 Successful initial request to the apiserver, version: v1.25.3
2024/06/30 16:32:21 Generating JWE encryption key
2024/06/30 16:32:21 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/06/30 16:32:21 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/06/30 16:32:21 Initializing JWE encryption key from synchronized object
2024/06/30 16:32:21 Creating in-cluster Sidecar client
2024/06/30 16:32:21 Serving insecurely on HTTP port: 9090
2024/06/30 16:32:24 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/06/30 16:32:54 Successful request to sidecar
2024/06/30 16:32:21 Starting overwatch


==> kubernetes-dashboard [671b7df479e5] <==
2024/06/30 14:37:13 Using namespace: kubernetes-dashboard
2024/06/30 14:37:13 Using in-cluster config to connect to apiserver
2024/06/30 14:37:13 Using secret token for csrf signing
2024/06/30 14:37:13 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/06/30 14:37:13 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/06/30 14:37:13 Successful initial request to the apiserver, version: v1.25.3
2024/06/30 14:37:13 Generating JWE encryption key
2024/06/30 14:37:13 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/06/30 14:37:13 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/06/30 14:37:13 Initializing JWE encryption key from synchronized object
2024/06/30 14:37:13 Creating in-cluster Sidecar client
2024/06/30 14:37:13 Serving insecurely on HTTP port: 9090
2024/06/30 14:37:16 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/06/30 14:37:46 Successful request to sidecar
2024/06/30 14:37:13 Starting overwatch


==> storage-provisioner [1764300c4198] <==
I0630 14:37:57.969767       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0630 14:37:57.983892       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0630 14:37:57.984510       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0630 14:38:15.399342       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0630 14:38:15.399516       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cf17ed09-1fab-4773-9dce-f6a3b443b4dd", APIVersion:"v1", ResourceVersion:"369542", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_4df62762-07be-4e87-aee1-f2f1a5d3cae6 became leader
I0630 14:38:15.399623       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_4df62762-07be-4e87-aee1-f2f1a5d3cae6!
I0630 14:38:15.501115       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_4df62762-07be-4e87-aee1-f2f1a5d3cae6!


==> storage-provisioner [52de638f12d4] <==
I0630 16:32:19.961977       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0630 16:32:19.968828       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0630 16:32:19.968861       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0630 16:32:37.373192       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0630 16:32:37.373387       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a3821635-9f0b-492f-a818-c7ce90c7bb39!
I0630 16:32:37.373486       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cf17ed09-1fab-4773-9dce-f6a3b443b4dd", APIVersion:"v1", ResourceVersion:"374815", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a3821635-9f0b-492f-a818-c7ce90c7bb39 became leader
I0630 16:32:37.473535       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a3821635-9f0b-492f-a818-c7ce90c7bb39!

